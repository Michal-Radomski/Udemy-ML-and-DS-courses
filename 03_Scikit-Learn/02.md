Linear regression in scikit-learn predicts continuous outcomes using a linear equation fitted to training data. Here's a
complete Jupyter Notebook example using the diabetes dataset, covering data prep, training, evaluation, and visualization.
[inria.github](https://inria.github.io/scikit-learn-mooc/python_scripts/linear_regression_in_sklearn.html)

## Import Libraries and Load Data

Start with essential imports and the built-in diabetes dataset for regression.

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Load diabetes dataset (predicts disease progression from features like BMI, BP)
diabetes = load_diabetes()
X = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)
y = diabetes.target  # Continuous target: disease progression measure

print(X.head())
print(f"Dataset shape: {X.shape}")
```

## Split Data and Train Model

Divide into train/test sets, then fit the LinearRegression model.
[scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)

```python
# Train-test split (80/20)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train model
model = LinearRegression()
model.fit(X_train, y_train)

# Model coefficients and intercept
print(f"Coefficients: {model.coef_[:5]}...")  # First 5 features
print(f"Intercept: {model.intercept_:.2f}")
```

## Make Predictions and Evaluate

Predict on test set and compute key metrics like MSE and R².
[codecademy](https://www.codecademy.com/article/linear-regression-with-scikit-learn-a-step-by-step-guide-using-python)

```python
# Predictions
y_pred = model.predict(X_test)

# Evaluation metrics
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse:.2f}")
print(f"R² Score: {r2:.3f}")
```

## Visualize Results

Plot actual vs predicted values for model inspection.
[inria.github](https://inria.github.io/scikit-learn-mooc/python_scripts/linear_regression_in_sklearn.html)

```python
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.6, color='blue')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Linear Regression: Actual vs Predicted')
plt.tight_layout()
plt.show()
```

This workflow scales to multiple features and integrates with pipelines from your prior preprocessing example. R² near 0.5
indicates moderate fit on this dataset; tune via feature selection for improvement.
[simplilearn](https://www.simplilearn.com/tutorials/scikit-learn-tutorial/sklearn-linear-regression-with-examples)

Polynomial regression extends linear regression by fitting higher-degree polynomials to capture non-linear relationships.
Here's a Jupyter Notebook example building on the diabetes dataset from linear regression, adding PolynomialFeatures for
quadratic terms. [data36](https://data36.com/polynomial-regression-python-scikit-learn/)

## Import Libraries and Load Data

Use the same diabetes setup, focusing on BMI feature for clear non-linear demo.

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.pipeline import Pipeline

# Load and select BMI feature only for visualization clarity
diabetes = load_diabetes()
X = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)
y = diabetes.target
X_bmi = X[['bmi']].values  # Single feature for polynomial demo

print(f"BMI range: {X_bmi.min():.1f} to {X_bmi.max():.1f}")
```

## Polynomial Features Pipeline

Create degree=2 polynomial features (bmi, bmi²) and fit linear model on them.
[data36](https://data36.com/polynomial-regression-python-scikit-learn/)

```python
# Split data
X_train, X_test, y_train, y_test = train_test_split(X_bmi, y, test_size=0.2, random_state=42)

# Pipeline: PolynomialFeatures -> LinearRegression
poly_model = Pipeline([
    ('poly', PolynomialFeatures(degree=2, include_bias=False)),
    ('linear', LinearRegression())
])

poly_model.fit(X_train, y_train)
y_poly_pred = poly_model.predict(X_test)

print(f"Polynomial R²: {r2_score(y_test, y_poly_pred):.3f}")
print(f"Polynomial RMSE: {np.sqrt(mean_squared_error(y_test, y_poly_pred)):.1f}")
```

## Compare Linear vs Polynomial

Linear model on raw BMI vs polynomial for better curve fit.
[data36](https://data36.com/polynomial-regression-python-scikit-learn/)

```python
# Linear baseline
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)
y_linear_pred = linear_model.predict(X_test)

print(f"Linear R²: {r2_score(y_test, y_linear_pred):.3f}")

# Plot comparison
sort_idx = X_test.flatten().argsort()
plt.figure(figsize=(10, 6))
plt.scatter(X_test, y_test, alpha=0.6, label='Actual')
plt.plot(X_test[sort_idx], y_linear_pred[sort_idx], 'r-', label='Linear')
plt.plot(X_test[sort_idx], y_poly_pred[sort_idx], 'g--', label='Polynomial (degree=2)')
plt.xlabel('BMI')
plt.ylabel('Disease Progression')
plt.legend()
plt.title('Linear vs Polynomial Regression')
plt.show()
```

Polynomial captures curvature better than linear on this feature, improving R². Scale to multiple features by using all
columns in PolynomialFeatures(degree=2). Higher degrees risk overfitting—use cross-validation.
[github](https://github.com/BaraSedih11/Polynomial-Regression)

Multiple linear regression uses the same LinearRegression class as simple linear regression but with multiple input features.
Here's a Jupyter Notebook example using the full diabetes dataset (10 features like BMI, blood pressure) to predict disease
progression. [geeksforgeeks](https://www.geeksforgeeks.org/machine-learning/multiple-linear-regression-with-scikit-learn/)

## Load Full Dataset

Use all features instead of single-feature versions from prior examples.

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Full diabetes dataset with 10 features
diabetes = load_diabetes()
X = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)
y = diabetes.target

print(f"Features: {X.columns.tolist()}")
print(f"Shape: {X.shape}")
print(X.head())
```

## Train Multiple Linear Model

Fit model using all 10 features simultaneously.
[geeksforgeeks](https://www.geeksforgeeks.org/machine-learning/multiple-linear-regression-with-scikit-learn/)

```python
# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Multiple linear regression (same class as simple linear)
model = LinearRegression()
model.fit(X_train, y_train)

# Coefficients show feature impact
coef_df = pd.DataFrame({
    'Feature': diabetes.feature_names,
    'Coefficient': model.coef_
}).sort_values('Coefficient', key=abs, ascending=False)

print(coef_df)
print(f"Intercept: {model.intercept_:.2f}")
```

## Evaluate and Visualize

Compare predictions across all features.
[geeksforgeeks](https://www.geeksforgeeks.org/machine-learning/multiple-linear-regression-with-scikit-learn/)

```python
# Predictions and metrics
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"MSE: {mse:.2f}")
print(f"R²: {r2:.3f}")

# Actual vs predicted scatter (multi-feature)
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('Actual Disease Progression')
plt.ylabel('Predicted')
plt.title('Multiple Linear Regression Results')
plt.show()
```

## Feature Importance Bar Plot

Visualize which features contribute most to predictions.
[geeksforgeeks](https://www.geeksforgeeks.org/machine-learning/multiple-linear-regression-with-scikit-learn/)

```python
plt.figure(figsize=(10, 6))
coef_df.plot(x='Feature', y='Coefficient', kind='barh', legend=False)
plt.title('Feature Coefficients (Absolute Impact)')
plt.xlabel('Coefficient Value')
plt.tight_layout()
plt.show()
```

This extends your prior linear regression—`bmi` and `s6` (blood serum) dominate predictions. R² improves with more relevant
features vs single-feature models.
[scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)
