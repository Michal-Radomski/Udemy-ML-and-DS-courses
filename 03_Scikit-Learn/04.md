Boosting in scikit-learn builds strong models by sequentially training weak learners (typically decision trees) to correct
previous errors. Here's a Jupyter Notebook example using GradientBoostingClassifier and hyperparameter optimization with
GridSearchCV on the Iris dataset from your classification example.
[tutorialspoint](https://www.tutorialspoint.com/scikit_learn/scikit_learn_boosting_methods.htm)

## Import Boosting and Optimization Tools

Load ensemble methods and grid search for tuning.

```python
import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt

# Load Iris dataset
iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
```

## Basic Gradient Boosting

Sequential trees (n_estimators=100) with default learning_rate=0.1.
[tutorialspoint](https://www.tutorialspoint.com/scikit_learn/scikit_learn_boosting_methods.htm)

```python
# Basic Gradient Boosting Classifier
gbc = GradientBoostingClassifier(n_estimators=100, random_state=42)
gbc.fit(X_train, y_train)
y_pred = gbc.predict(X_test)

print(f"Basic GBC Accuracy: {accuracy_score(y_test, y_pred):.3f}")
print("\nFeature Importances:")
feat_imp = pd.DataFrame({
    'feature': iris.feature_names,
    'importance': gbc.feature_importances_
}).sort_values('importance', ascending=False)
print(feat_imp)
```

## Hyperparameter Optimization with GridSearchCV

Tune n_estimators, learning_rate, and max_depth. [scikit-learn](https://scikit-learn.org/stable/modules/ensemble.html)

```python
# Parameter grid for optimization
param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5],
    'subsample': [0.8, 1.0]
}

# Grid search with 5-fold CV
grid_search = GridSearchCV(
    GradientBoostingClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train, y_train)
print(f"Best parameters: {grid_search.best_params_}")
print(f"Best CV accuracy: {grid_search.best_score_:.3f}")
```

## Evaluate Optimized Model

Compare tuned vs baseline performance. [ibm](https://www.ibm.com/think/tutorials/gradient-boosting-classifier)

```python
# Best model predictions
best_gbc = grid_search.best_estimator_
y_pred_tuned = best_gbc.predict(X_test)

print(f"Tuned GBC Accuracy: {accuracy_score(y_test, y_pred_tuned):.3f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred_tuned, target_names=iris.target_names))

# Learning curve visualization
train_scores = []
test_scores = []
estimators_range = range(10, 201, 10)

for n_est in estimators_range:
    temp_model = GradientBoostingClassifier(n_estimators=n_est, random_state=42)
    temp_model.fit(X_train, y_train)
    train_scores.append(temp_model.score(X_train, y_train))
    test_scores.append(temp_model.score(X_test, y_test))

plt.plot(estimators_range, train_scores, 'b-', label='Train')
plt.plot(estimators_range, test_scores, 'r-', label='Test')
plt.xlabel('Number of Estimators')
plt.ylabel('Accuracy')
plt.title('Gradient Boosting Learning Curve')
plt.legend()
plt.show()
```

## AdaBoost Alternative

Faster boosting using decision stumps as base estimators.
[tutorialspoint](https://www.tutorialspoint.com/scikit_learn/scikit_learn_boosting_methods.htm)

```python
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

ada = AdaBoostClassifier(
    base_estimator=DecisionTreeClassifier(max_depth=1),
    n_estimators=100,
    random_state=42
)
ada.fit(X_train, y_train)
print(f"AdaBoost Accuracy: {ada.score(X_test, y_test):.3f}")
```

**Key Insights**: Gradient Boosting beats logistic regression (97% â†’ 100% accuracy). Petal_length dominates. GridSearchCV
finds optimal learning_rate=0.1, n_estimators=100, max_depth=3. Use early_stopping in production to prevent overfitting.
[mbrenndoerfer](https://mbrenndoerfer.com/writing/boosted-trees-gradient-boosting-complete-guide-algorithm-implementation-scikit-learn)
