Hyperparameters in scikit-learn are configuration settings for machine learning models that control the learning process but
are not learned from the data during training. They must be set before fitting the model and are often tuned to optimize
performance on a specific dataset.
[inria.github](https://inria.github.io/scikit-learn-mooc/python_scripts/parameter_tuning_manual.html)

## Key Characteristics

Hyperparameters are passed as arguments to an estimator's constructor, such as `C` (regularization strength) in SVM or
Logistic Regression, `kernel` and `gamma` in SVM, or `n_estimators` in random forests. Unlike fitted parameters (e.g.,
`model.coef_`), they do not have a trailing underscore and remain fixed unless explicitly changed. They are model-specific
and dataset-dependent, requiring optimization via techniques like grid search or cross-validation.
[scikit-learn](https://scikit-learn.org/stable/modules/grid_search.html)

## Viewing and Setting

Use `model.get_params()` to list all hyperparameters and their values, which returns a dictionary (e.g., `classifier__C` in
pipelines). Modify them with `model.set_params(**params)`, such as `model.set_params(C=1e-3)`, before refitting. This works
for single estimators or pipelines, where names follow `<step>__<param>` format.
[inria.github](https://inria.github.io/scikit-learn-mooc/python_scripts/parameter_tuning_manual.html)

## Tuning Methods

Scikit-learn provides tools like `GridSearchCV` for exhaustive search over a parameter grid, `RandomizedSearchCV` for random
sampling, and others for efficient optimization. For example, tune `C` values `[1e-3, 1e-2, 1]` via cross-validation to find
the best accuracy. Always validate on held-out data to avoid overfitting during selection.
[pyimagesearch](https://pyimagesearch.com/2021/05/17/introduction-to-hyperparameter-tuning-with-scikit-learn-and-python/)
